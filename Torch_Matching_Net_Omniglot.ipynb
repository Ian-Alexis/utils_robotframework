{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Torch Matching Net - Omniglot.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ian-Alexis/utils_robotframework/blob/main/Torch_Matching_Net_Omniglot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "nsnJVyFjvwDB"
      },
      "cell_type": "markdown",
      "source": [
        "# One Shot learning on Omniglot Dataset\n",
        "\n",
        "Our goal as part of this notebook is to solve the [one-shot learning problem](https://en.wikipedia.org/wiki/One-shot_learning) for the [omniglot dataset](https://github.com/brendenlake/omniglot). We aim to learn a model that given a set of images, its associated labels and a target image, we should be able to correctly assign the label of the target image from the provided labels.\n",
        "##Example of a one shot classification task\n",
        "![One shot Task](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAcoAAAHQCAYAAAAh0SohAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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%0AMCBRAgBgQKIEAMCARAkAgAGJEgAAg/8HrjWlqzde0WoAAAAASUVORK5CYII=)\n",
        "\n",
        "Here our goal would be to match the 2nd Image of the first row to the single image shown in last row (as indicated by the labels)"
      ]
    },
    {
      "metadata": {
        "id": "sqluy6IhG9wh"
      },
      "cell_type": "markdown",
      "source": [
        "#Installing Requirements and Imports\n",
        "\n",
        "We will be using **PyTorch** for defining and training our model and **comet.ml** for logging our stats"
      ]
    },
    {
      "metadata": {
        "id": "ot-iNuia-VUt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fd4dc09-6c99-4d9f-8fb3-f6241d634ba3"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision\n",
        "!pip3 install comet_ml"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting comet_ml\n",
            "  Downloading comet_ml-3.47.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting everett<3.2.0,>=1.0.1 (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n",
            "  Downloading everett-3.1.0-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (4.23.0)\n",
            "Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (5.9.5)\n",
            "Collecting python-box<7.0.0 (from comet_ml)\n",
            "  Downloading python_box-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: requests-toolbelt>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (2.32.3)\n",
            "Collecting semantic-version>=2.8.0 (from comet_ml)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: sentry-sdk>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (2.19.2)\n",
            "Collecting simplejson (from comet_ml)\n",
            "  Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (2.2.3)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (1.17.0)\n",
            "Collecting wurlitzer>=1.0.2 (from comet_ml)\n",
            "  Downloading wurlitzer-3.1.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting dulwich!=0.20.33,>=0.20.6 (from comet_ml)\n",
            "  Downloading dulwich-0.22.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (13.9.4)\n",
            "Collecting configobj (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n",
            "  Downloading configobj-5.0.9-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.22.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet_ml) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet_ml) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet_ml) (4.12.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet_ml) (0.1.2)\n",
            "Downloading comet_ml-3.47.5-py3-none-any.whl (710 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m710.5/710.5 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dulwich-0.22.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (981 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.8/981.8 kB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading everett-3.1.0-py2.py3-none-any.whl (35 kB)\n",
            "Downloading python_box-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading wurlitzer-3.1.1-py3-none-any.whl (8.6 kB)\n",
            "Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading configobj-5.0.9-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: everett, wurlitzer, simplejson, semantic-version, python-box, dulwich, configobj, comet_ml\n",
            "  Attempting uninstall: python-box\n",
            "    Found existing installation: python-box 7.3.0\n",
            "    Uninstalling python-box-7.3.0:\n",
            "      Successfully uninstalled python-box-7.3.0\n",
            "Successfully installed comet_ml-3.47.5 configobj-5.0.9 dulwich-0.22.6 everett-3.1.0 python-box-6.1.0 semantic-version-2.10.0 simplejson-3.19.3 wurlitzer-3.1.1\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "fY3hj2pE-HAP"
      },
      "cell_type": "code",
      "source": [
        "from comet_ml import Experiment\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3m4zN9QeHFo5"
      },
      "cell_type": "markdown",
      "source": [
        "#Model Architecture"
      ]
    },
    {
      "metadata": {
        "id": "j6stLGjEavMT"
      },
      "cell_type": "markdown",
      "source": [
        "## Convolutional Block\n",
        "A Standard convolutional block which follows: <br>\n",
        "Image --> Convolutional layer --> ReLU Activation --> Batch Normalization --> Pooling Layer --> Dropout.<br>\n",
        "The dropout layer has been added for improved regularization."
      ]
    },
    {
      "metadata": {
        "id": "ZuPjY1Eg8roX"
      },
      "cell_type": "code",
      "source": [
        "class ConvLayerWithBatchNorm(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels=64, kernel_size=3, padding=1, dropout_probality=0.2):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding)\n",
        "    self.ReLU = nn.ReLU()\n",
        "    self.batch_norm_layer = nn.BatchNorm2d(out_channels)\n",
        "    self.maxpool = nn.MaxPool2d(2, 2)\n",
        "    self.dropout = nn.Dropout(dropout_probality) # Dropout to add regularization and improve model generalization\n",
        "\n",
        "  def forward(self, X):\n",
        "    x = self.conv(X)\n",
        "    x = self.ReLU(x)\n",
        "    x = self.batch_norm_layer(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.dropout(x)\n",
        "    return x"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0XdMt0F4f6oA"
      },
      "cell_type": "markdown",
      "source": [
        "##Embedding Layer\n",
        "This is the embedding model, which is a stack of 4 of the above `ConvLayerWithBatchNorm` layers. <br>\n",
        "An addtional fully connected layer is added at the end along with dropout"
      ]
    },
    {
      "metadata": {
        "id": "tP0nQ38G-SBB"
      },
      "cell_type": "code",
      "source": [
        "class ConvEmbedding(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels=1, embedding_size=256, dropout_probality=0.2):\n",
        "    super().__init__()\n",
        "    self.conv1 = ConvLayerWithBatchNorm(in_channels, 64, dropout_probality=dropout_probality)\n",
        "    self.conv2 = ConvLayerWithBatchNorm(64, 64, dropout_probality=dropout_probality)\n",
        "    self.conv3 = ConvLayerWithBatchNorm(64, 64, dropout_probality=dropout_probality)\n",
        "    self.conv4 = ConvLayerWithBatchNorm(64, 64, dropout_probality=dropout_probality)\n",
        "    self.dense = nn.Linear(64, embedding_size)\n",
        "    self.dropout = nn.Dropout(dropout_probality) # Dropout to add regularization and improve model generalization\n",
        "    self.embedding_size = embedding_size\n",
        "\n",
        "  def forward(self, X):\n",
        "    # Input shape is (batch_size, 1, 28, 28)\n",
        "    x = self.conv1(X) # x's Shape (batch_size, 64, 14, 14)\n",
        "    x = self.conv2(x) # x's Shape (batch_size, 64, 7, 7)\n",
        "    x = self.conv3(x) # x's Shape (batch_size, 64, 3, 3)\n",
        "    x = self.conv4(x) # x's Shape (batch_size, 64, 1, 1)\n",
        "    x = x.view(x.size()[0], -1) # x's Shape (batch_size, 64)\n",
        "    x = self.dense(x) # x's Shape (batch_size, embedding_size)\n",
        "    x = self.dropout(x) # x's Shape (batch_size, embedding_size) with a few activations flipped to 0\n",
        "    return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cejO4ZTSnb7p"
      },
      "cell_type": "markdown",
      "source": [
        "##  The Fully Conditional Embedding - Target Image\n",
        "Now we have the fully conditional embedding layer for our target image. This allows us to obtain an embedding for our target image in the context of the support images.\n",
        "\n",
        "One significant change in our implementation is that instead of concatenating the hidden state and our attended output, we choose to add them instead. This is due to to the available LSTMCell has hidden weights of shape (hidden_size, hidden_size) but in order to take the concatenated input as the hidden state, we must construct our custom LSTMCell which has hidden weights of size (2xhidden_size, hidden_size).\n",
        "\n",
        "This can be found mentioned by one of the authors Oriol Vinayls [on machine learning subreddit here](https://www.reddit.com/r/MachineLearning/comments/6efl5g/d_order_matters_attention_mechanisms/)"
      ]
    },
    {
      "metadata": {
        "id": "RSInVD2Gtk--"
      },
      "cell_type": "code",
      "source": [
        "class FullyConditionalEmbeddingTargetImage(nn.Module):\n",
        "\n",
        "  def __init__(self, embedding_size, processing_steps=10):\n",
        "    super().__init__()\n",
        "    self.lstm_cell = torch.nn.LSTMCell(embedding_size, embedding_size)\n",
        "    self.processing_steps = processing_steps\n",
        "    self.embedding_size = embedding_size\n",
        "    self.attn_softmax = nn.Softmax(dim=1)\n",
        "  def forward(self, target_image_encoded, support_images_encoded):\n",
        "    batch_size, num_images, _ = support_images_encoded.shape\n",
        "\n",
        "#     hidden_state_prev = torch.zeros(batch_size, self.embedding_size).to(device)\n",
        "    cell_state_prev = torch.zeros(batch_size, self.embedding_size).to(device)\n",
        "    hidden_state_prev = torch.sum(support_images_encoded, dim=1) / num_images\n",
        "    for i in range(self.processing_steps):\n",
        "      hidden_out, cell_out = self.lstm_cell(target_image_encoded, (hidden_state_prev, cell_state_prev))\n",
        "      hidden_out = hidden_out + target_image_encoded\n",
        "      attn = self.attn_softmax(torch.bmm(support_images_encoded, hidden_out.unsqueeze(2)))\n",
        "      attended_values = torch.sum(attn * support_images_encoded, dim=1)\n",
        "      hidden_state_prev = hidden_out + attended_values\n",
        "      cell_state_prev = cell_out\n",
        "\n",
        "    return hidden_out"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vbyJSn3j5H0A"
      },
      "cell_type": "markdown",
      "source": [
        "## The Fully Conditional Embedding - Support Images\n",
        "\n",
        "Here we have the the fully conditional embeddings for our support images, this is similar to our previous layer for the target image except for the difference that it is just a simple bi-directional LSTM where we just pass our support images sequence.\n",
        "\n",
        "Our new embeddings are going to be the LSTM's **Forward Activation + Backward Activation + Support Image Embedding**"
      ]
    },
    {
      "metadata": {
        "id": "AEmsFcZMr0Hq"
      },
      "cell_type": "code",
      "source": [
        "class FullyConditionalEmbeddingSupportImages(nn.Module):\n",
        "\n",
        "  def __init__(self, embedding_size):\n",
        "    super().__init__()\n",
        "    self.embedding_size = embedding_size\n",
        "    self.bidirectionalLSTM = nn.LSTM(input_size=embedding_size, hidden_size=embedding_size, bidirectional=True, batch_first=True)\n",
        "\n",
        "  def initialize_hidden(self, batch_size):\n",
        "    #Initialize the states needed for our bi-directional LSTM\n",
        "    hidden_state = torch.zeros(2, batch_size, self.embedding_size).to(device)\n",
        "    cell_state = torch.zeros(2, batch_size, self.embedding_size).to(device)\n",
        "    return (hidden_state, cell_state)\n",
        "\n",
        "  def forward(self, support_embeddings):\n",
        "    batch_size, num_images, _ = support_embeddings.shape\n",
        "    # Initialize states\n",
        "    lstm_states = self.initialize_hidden(batch_size)\n",
        "    # Get the LSTM Outputs\n",
        "    support_embeddings_contextual, internal_states = self.bidirectionalLSTM(support_embeddings, lstm_states)\n",
        "    # Get the forward and backward outputs\n",
        "    support_embeddings_contextual = support_embeddings_contextual.view(batch_size, num_images, 2, self.embedding_size)\n",
        "    # Add the forward and backward outputs\n",
        "    support_embeddings_contextual = torch.sum(support_embeddings_contextual, dim=2)\n",
        "    # Add the skip connection to our output\n",
        "    support_embeddings_contextual = support_embeddings_contextual + support_embeddings\n",
        "    return support_embeddings_contextual"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o82Te4-sgQ-C"
      },
      "cell_type": "markdown",
      "source": [
        "##Cosine Distance Module\n",
        "A cosine distance module that allows us to compute the cosine distance of our target image with each of the support images"
      ]
    },
    {
      "metadata": {
        "id": "MO31uRMo-QtZ"
      },
      "cell_type": "code",
      "source": [
        "class CosineDistance(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, target_image, support_images):\n",
        "    support_images_normed = F.normalize(support_images, p=2, dim=2)\n",
        "    # the 'p=2' param represents squared norm\n",
        "    target_image_normed = F.normalize(target_image, p=2, dim=1)\n",
        "    target_image_normed = target_image_normed.unsqueeze(dim=1).permute(0, 2, 1)\n",
        "    # This will cause the dimensions to be [5, 64, 1]\n",
        "    similarities = torch.bmm(support_images_normed, target_image.unsqueeze(1).permute(0, 2, 1))\n",
        "    # torch.bmm = batch matrix multiply\n",
        "    # [5, 20, 64] @ [5, 64, 1]\n",
        "    # the output shape is [5, 20, 1]\n",
        "    similarities = similarities.squeeze(dim=2)\n",
        "    # remove last dimension\n",
        "    return similarities"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K2REH_O-gaGk"
      },
      "cell_type": "markdown",
      "source": [
        "##One-Hot Converter Module\n",
        "A helper class that allows us to convert labels into one-hot vectors"
      ]
    },
    {
      "metadata": {
        "id": "SrBPpY-nyjFh"
      },
      "cell_type": "code",
      "source": [
        "# Taken from @activatedgeeks's answer from https://stackoverflow.com/questions/44461772/creating-one-hot-vector-from-indices-given-as-a-tensor\n",
        "class ConvertOneHot(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, labels, num_classes):\n",
        "    batch_size, num_images, _ = labels.size()\n",
        "    one_hot_labels = torch.Tensor(batch_size, num_images, num_classes).to(labels.device).float().zero_()\n",
        "    return one_hot_labels.scatter(2, labels, 1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RKHpVs7CggJU"
      },
      "cell_type": "markdown",
      "source": [
        "##Matching Network\n",
        "The orchestrating module that builds the entire matching network.<br> We implementing the matching networks described in this [paper](https://arxiv.org/pdf/1606.04080.pdf)<br>\n",
        "\n",
        "For an overview of the paper read up on this blogpost from [Adrian Colyer on it](https://blog.acolyer.org/2017/01/03/matching-networks-for-one-shot-learning/)\n",
        "\n",
        "**Input to the Network**: *Support Images*, *Support Labels* and *Target Image*<br>\n",
        "**Output of the Network**: *One-Hot encoded Target label*<br>\n",
        "\n",
        "1. The model operates by first getting the embeddings for both support images and our target image via the `ConvEmbedding` model.\n",
        "* Then we calculate the consine distance between the target image embedding and each of the support image embedding.\n",
        "* We squash the distances via a softmax function. This acts as our attention weights\n",
        "* We multiply the attention weights with our one-hot encoded labels and sum them in order to get our predicted one-hot label.\n",
        "* We then measure the cross entropy loss of our predicted label and the expected target label and backpropagate.\n",
        "\n",
        "Drew Heavy inspiration from the following Matching Networks implementations:\n",
        "\n",
        "\n",
        "1.   BoyuanJiang's implementation - https://github.com/BoyuanJiang/matching-networks-pytorch\n",
        "2.   activatedgeek's implementation - https://github.com/activatedgeek/Matching-Networks\n",
        "3.   Mark's implementation for the Full Context Embeddings - https://github.com/markdtw/matching-networks\n"
      ]
    },
    {
      "metadata": {
        "id": "vKzh31O3-f6I"
      },
      "cell_type": "code",
      "source": [
        "class MatchingNet(nn.Module):\n",
        "\n",
        "  def __init__(self, image_shape, embedding_size=256, dropout_probality=0.2, use_fce=True):\n",
        "    super().__init__()\n",
        "    self.attn = nn.Softmax(dim=1)\n",
        "    self.embedding = ConvEmbedding(embedding_size=embedding_size, dropout_probality=dropout_probality)\n",
        "    self.distance = CosineDistance()\n",
        "    self.use_fce = use_fce\n",
        "    self.onehotconverter = ConvertOneHot()\n",
        "    if self.use_fce:\n",
        "      self.full_conditional_embedding_support = FullyConditionalEmbeddingSupportImages(embedding_size=embedding_size)\n",
        "      self.full_conditional_embedding_target = FullyConditionalEmbeddingTargetImage(embedding_size=embedding_size)\n",
        "    self.image_shape = image_shape\n",
        "\n",
        "  def forward(self, support_images, support_labels, target_image):\n",
        "\n",
        "    batch_size, num_images, _ = support_labels.size()\n",
        "\n",
        "    # Get the image encodings from convolutional embedding\n",
        "    target_image_encoded = self.embedding(target_image)\n",
        "    support_images_encoded = self.embedding(support_images.view(-1, *self.image_shape)).view(-1, num_images, self.embedding.embedding_size)\n",
        "\n",
        "    if self.use_fce:\n",
        "      # Get the support images embedding with context\n",
        "      support_images_encoded = self.full_conditional_embedding_support(support_images_encoded)\n",
        "\n",
        "      # Get the target image embedding with context\n",
        "      target_image_encoded = self.full_conditional_embedding_target(target_image_encoded, support_images_encoded)\n",
        "\n",
        "    # Get the cosine distances between target image and the support images\n",
        "    distances = self.distance(target_image_encoded, support_images_encoded)\n",
        "\n",
        "    # Get the attention value based on the distances\n",
        "    attention = self.attn(distances)\n",
        "\n",
        "    # Convert the labels into one hot vectors\n",
        "    support_set_one_hot_labels = self.onehotconverter(support_labels, num_images)\n",
        "\n",
        "    # Get the prediction logits by attention * one-hot-labels (automatically summed due to the unsqueeze operation)\n",
        "    prediction_logits = torch.bmm(attention.unsqueeze(1), support_set_one_hot_labels).squeeze()\n",
        "\n",
        "    # Get the final labels for predictions\n",
        "    _, prediction_labels = torch.max(prediction_logits, 1)\n",
        "    return prediction_logits, prediction_labels"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XNtPR5jgHKtY"
      },
      "cell_type": "markdown",
      "source": [
        "#Testing the model\n",
        "\n",
        "We just run the model via random data over a few epochs to ensure that it is able to learn something and ensure that the gradients propagate correctly i.e catch any silly mistakes like dimension mismatch etc"
      ]
    },
    {
      "metadata": {
        "id": "GaYlaW7KG4Zt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e59bb7ef-58ce-4d5c-87ca-b194ed686424"
      },
      "cell_type": "code",
      "source": [
        "img_shape = (1, 28, 28)\n",
        "matching_net_trial = MatchingNet(img_shape, dropout_probality=0.1, use_fce=False)\n",
        "print(\"Model Summary\")\n",
        "print(matching_net_trial)\n",
        "epochs = 10\n",
        "\n",
        "support_images = torch.rand(32, 20, *img_shape)\n",
        "target_image = torch.rand(32, *img_shape)\n",
        "support_labels = torch.LongTensor(32, 20, 1) % 20\n",
        "target_labels = torch.LongTensor(32) % 20\n",
        "\n",
        "matching_net_trial.to(device)\n",
        "support_images = support_images.to(device)\n",
        "support_labels = support_labels.to(device)\n",
        "target_image = target_image.to(device)\n",
        "target_labels = target_labels.to(device)\n",
        "optimizer = torch.optim.Adam(matching_net_trial.parameters(), lr=0.001)\n",
        "for epoch in range(epochs):\n",
        "  logits, predictions = matching_net_trial(support_images, support_labels,target_image)\n",
        "  loss = F.cross_entropy(logits, target_labels)\n",
        "  print(loss.item())\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Summary\n",
            "MatchingNet(\n",
            "  (attn): Softmax(dim=1)\n",
            "  (embedding): ConvEmbedding(\n",
            "    (conv1): ConvLayerWithBatchNorm(\n",
            "      (conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (ReLU): ReLU()\n",
            "      (batch_norm_layer): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (conv2): ConvLayerWithBatchNorm(\n",
            "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (ReLU): ReLU()\n",
            "      (batch_norm_layer): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (conv3): ConvLayerWithBatchNorm(\n",
            "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (ReLU): ReLU()\n",
            "      (batch_norm_layer): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (conv4): ConvLayerWithBatchNorm(\n",
            "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (ReLU): ReLU()\n",
            "      (batch_norm_layer): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (dense): Linear(in_features=64, out_features=256, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (distance): CosineDistance()\n",
            "  (onehotconverter): ConvertOneHot()\n",
            ")\n",
            "2.265655279159546\n",
            "2.265655040740967\n",
            "2.265655040740967\n",
            "2.265655279159546\n",
            "2.265655279159546\n",
            "2.265655040740967\n",
            "2.265655279159546\n",
            "2.265655279159546\n",
            "2.265655040740967\n",
            "2.265655279159546\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "PfgmRLj0HSG3"
      },
      "cell_type": "markdown",
      "source": [
        "#Gathering the Data\n",
        "\n",
        "We fetch the data from github repository and unzip the train and evaluation zip files"
      ]
    },
    {
      "metadata": {
        "id": "cLtpEjzOIU-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "400cf444-d1b2-4f41-e963-dd92c6b623e8"
      },
      "cell_type": "code",
      "source": [
        "!git clone --depth 1 https://github.com/brendenlake/omniglot.git\n",
        "!unzip -qq omniglot/python/images_background.zip\n",
        "!unzip -qq omniglot/python/images_evaluation.zip"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'omniglot'...\n",
            "remote: Enumerating objects: 38, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 38 (delta 5), reused 30 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (38/38), 85.74 MiB | 41.91 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cLMlgkkuKtvE"
      },
      "cell_type": "markdown",
      "source": [
        "Helper functions to read and rotate images"
      ]
    },
    {
      "metadata": {
        "id": "PdbvPNTh0n6Y"
      },
      "cell_type": "code",
      "source": [
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "\n",
        "def read_image(path, size, angle=0):\n",
        "  img = io.imread(path, as_grey=True)\n",
        "  img = transform.resize(img, size, mode='constant')\n",
        "#   img = 1 - np.expand_dims(img, 0)\n",
        "  img = np.expand_dims(img, 0)\n",
        "  return img\n",
        "\n",
        "def rotate(img, angle):\n",
        "  return np.expand_dims(transform.rotate(np.squeeze(img), angle), 0)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qBwYPG4K_oxZ"
      },
      "cell_type": "code",
      "source": [
        "PATH = 'images_background'\n",
        "EVALUATION_PATH = 'images_evaluation'"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-8Z3b3H2K04Z"
      },
      "cell_type": "markdown",
      "source": [
        "We recursively go over each Language and Alphabet and extract the images of each Letter present in them into a dictionary.<br>\n",
        "We also extract the images here into numpy arrays for both the \"Train\" and \"Evaluation\" sets."
      ]
    },
    {
      "metadata": {
        "id": "7DL1jQoO_G0-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "4d17c47f-f8bd-432f-bd82-94bd435baf35"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def load_alphabet(path):\n",
        "  lang_dict = []\n",
        "  for alphabet in os.listdir(path):\n",
        "    alphabet_path = os.path.join(path, alphabet)\n",
        "    for letter in os.listdir(alphabet_path):\n",
        "      current_letter_dict = {\"alphabet\": alphabet, \"letter\":letter, \"images\":[], \"image_mat\": []}\n",
        "      letter_path = os.path.join(alphabet_path, letter)\n",
        "      for letter_image in os.listdir(letter_path):\n",
        "        letter_image_path = os.path.join(letter_path, letter_image)\n",
        "        current_letter_dict[\"images\"].append(letter_image_path)\n",
        "        img = read_image(letter_image_path, (28, 28))\n",
        "        current_letter_dict[\"image_mat\"].append(img)\n",
        "      lang_dict.append(current_letter_dict)\n",
        "  return lang_dict\n",
        "\n",
        "train_dict = load_alphabet(PATH)\n",
        "test_dict = load_alphabet(EVALUATION_PATH)\n",
        "print(f\"found letters in Training set - {len(train_dict)}\")\n",
        "print(f\"found letters in Testing set - {len(test_dict)}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "PillowPlugin.read() got an unexpected keyword argument 'as_grey'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-6dd37b204018>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlang_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtrain_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_alphabet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mtest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_alphabet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEVALUATION_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"found letters in Training set - {len(train_dict)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-6dd37b204018>\u001b[0m in \u001b[0;36mload_alphabet\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mletter_image_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mletter_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mletter_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mcurrent_letter_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"images\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mletter_image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mletter_image_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mcurrent_letter_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_mat\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mlang_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_letter_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-53d4c52aa28c>\u001b[0m in \u001b[0;36mread_image\u001b[0;34m(path, size, angle)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_grey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'constant'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#   img = 1 - np.expand_dims(img, 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skimage/io/_io.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_or_url_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imread'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Could not find the plugin \"{plugin}\" for {kind}.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skimage/io/_plugins/imageio_plugin.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageio_imread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageio_imread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'WRITEABLE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imageio/v3.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(uri, index, plugin, extension, format_hint, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mimopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_kwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimg_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: PillowPlugin.read() got an unexpected keyword argument 'as_grey'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Z-Y4rWPGLSJJ"
      },
      "cell_type": "markdown",
      "source": [
        "We also define an augmentation function that addtionally generates 4 rotations of each alphabet thus increasing the number of classes we possess by a factor of 4. There is still some issue with the model when it utilizes augmented data so for now we will not be using it."
      ]
    },
    {
      "metadata": {
        "id": "JqwrW6OdcDWX"
      },
      "cell_type": "code",
      "source": [
        "def augment_dataset(dataset):\n",
        "  augmented_dataset = []\n",
        "  for letter in dataset:\n",
        "    for angle in range(4):\n",
        "      current_letter_dict = {\"alphabet\": letter[\"alphabet\"],\n",
        "                             \"letter\":letter[\"letter\"],\n",
        "                             \"images\":letter[\"images\"],\n",
        "                             \"image_mat\": []}\n",
        "      for image in letter[\"image_mat\"]:\n",
        "        rotated_image = rotate(image, angle*90)\n",
        "        current_letter_dict[\"image_mat\"].append(rotated_image)\n",
        "      augmented_dataset.append(current_letter_dict)\n",
        "  return augmented_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mUKSdNAIHWHS"
      },
      "cell_type": "markdown",
      "source": [
        "##Splitting data into Train/Dev/Test Sets\n",
        "Using sklearn's `train_test_split` function we create 3 sets of data.\n",
        "\n",
        "\n",
        "1.   **Training Set** - Used to train our model\n",
        "2.   **Dev Set** - Used to evaluate our model during the training process in order to perform hyperparameter tuning\n",
        "3.   **Testing Set** - Used to finally evaluate our model\n",
        "\n",
        "Before generating the above three sets, we first combine the provided Train and Test sets since they seem to be coming from different distributions and the algorithm was performing poorly when using the provided Train/Test Split.\n",
        "\n",
        "We set aside 25% of total data for testing set.\n",
        "\n",
        "From the remaining 75% of data we again split the data in 9:1 ratio. 67.5% of total data for training set and 7.5% of total data for dev set\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Um5ZxBm0G-IM"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "random_state = 50\n",
        "total_data = train_dict + test_dict\n",
        "training_dict, testing_dict = train_test_split(total_data, test_size=0.25, random_state=random_state)\n",
        "training_dict, dev_dict = train_test_split(training_dict, test_size=0.1, random_state=random_state)\n",
        "\n",
        "print(f\"Total Dataset size - {len(total_data)}\")\n",
        "print(f\"Training Dataset size - {len(training_dict)}\")\n",
        "print(f\"Dev Dataset size - {len(dev_dict)}\")\n",
        "print(f\"Testing Dataset size - {len(testing_dict)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bl74JkPedQQL"
      },
      "cell_type": "code",
      "source": [
        "# augmented_train_dict = augment_dataset(training_dict)\n",
        "# augmented_dev_dict = augment_dataset(dev_dict)\n",
        "augmented_train_dict = training_dict\n",
        "augmented_dev_dict = dev_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "83ghW7TYHgbX"
      },
      "cell_type": "markdown",
      "source": [
        "##Creating the tasks for our Model"
      ]
    },
    {
      "metadata": {
        "id": "XKBmrVe2NNFI"
      },
      "cell_type": "markdown",
      "source": [
        "We define a function that takes one of the datasets and creates a ***N-Way*** one shot task where we choose ***N*** different letters (i.e Unique Characters from our dataset) and take one random image from each of them. Additionally one extra Image is chosen from one of the ***N***  chosen letters (different from the random image chosen). This now constitues our one shot task.<br>\n",
        "If our model is able to successfully return the label of the image from the support set which best matches the target image, then our model is successful."
      ]
    },
    {
      "metadata": {
        "id": "PC8nJbNVAajT"
      },
      "cell_type": "code",
      "source": [
        "img_size = (28, 28)\n",
        "\n",
        "def make_oneshot_task(dataset, nway=20):\n",
        "\n",
        "  # Choose nway random letters from dataset\n",
        "  letter_choices = np.random.choice(len(dataset), nway, replace=False)\n",
        "\n",
        "  # Placeholders for our support dataset\n",
        "  X = np.empty((nway, 1, *img_size))\n",
        "  y = np.empty((nway), dtype=int)\n",
        "\n",
        "  # Choose random letter from support set to be the target letter\n",
        "  random_target = np.random.choice(nway, size=1)\n",
        "  batch_letters = [dataset[k] for k in letter_choices]\n",
        "  required_letter = None\n",
        "  required_class = -1\n",
        "  for i, letter in enumerate(batch_letters):\n",
        "      # Choose random image from each letter\n",
        "      letter_index = np.random.randint(len(letter[\"images\"]))\n",
        "      X[i,] = letter[\"image_mat\"][letter_index]\n",
        "      y[i] = i\n",
        "      if(i == random_target[0]):\n",
        "        # Fetching a different image for our target image from chosen letter\n",
        "        required_index = (letter_index + np.random.randint(1, len(letter[\"images\"]))) % len(letter[\"images\"])\n",
        "        required_letter = letter[\"image_mat\"][required_index]\n",
        "        required_class = i\n",
        "  return X, y, required_letter, required_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nNgaQbfqLhVP"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualizing one-shot tasks"
      ]
    },
    {
      "metadata": {
        "id": "tn8TJKBQM0W6"
      },
      "cell_type": "markdown",
      "source": [
        "Helper Visualization function to ensure our datasets and task generation functions are valid."
      ]
    },
    {
      "metadata": {
        "id": "BOJ2UXXd_lqa"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_oneshot_task(X, y, required_letter, required_class):\n",
        "  fig=plt.figure(figsize=(8, 8))\n",
        "  columns = 5\n",
        "  rows = X.shape[0]//columns + 1\n",
        "  for i in range(1, X.shape[0] + 1):\n",
        "    img = X[i-1]\n",
        "    fig.add_subplot(rows, columns, i).set_title(y[i-1])\n",
        "    plt.axis('off')\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(img.reshape(*img_size))\n",
        "  fig.add_subplot(rows, columns, X.shape[0] + 3).set_title(required_class)\n",
        "  plt.imshow(required_letter.reshape(*img_size))\n",
        "  plt.axis('off')\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cRhUKYzx1mhc"
      },
      "cell_type": "code",
      "source": [
        "# Visualize our training dataset\n",
        "visualize_oneshot_task(*make_oneshot_task(augmented_train_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bj29koiNJkxD"
      },
      "cell_type": "code",
      "source": [
        "# Visualize our validation/dev dataset\n",
        "visualize_oneshot_task(*make_oneshot_task(augmented_dev_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1_geXRMyJo4L"
      },
      "cell_type": "code",
      "source": [
        "# Visualize our test dataset\n",
        "visualize_oneshot_task(*make_oneshot_task(testing_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QESv_nGFOfot"
      },
      "cell_type": "markdown",
      "source": [
        "Create a PyTorch `Dataset` class that allows us to leverage multiprocessing while loading our data between batches."
      ]
    },
    {
      "metadata": {
        "id": "qsNZHRbbX1it"
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "class OneShotDataset(Dataset):\n",
        "\n",
        "  def __init__(self, data_dict, iterations=1000):\n",
        "    self.data_dict = data_dict\n",
        "    self.length = iterations\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    support_images, support_labels, target_image, target_label = make_oneshot_task(self.data_dict)\n",
        "    return (torch.from_numpy(support_images).float(),\n",
        "            torch.from_numpy(support_labels).long().unsqueeze(-1),\n",
        "            torch.from_numpy(target_image).float(),\n",
        "            target_label)\n",
        "  def __len__(self):\n",
        "    return self.length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_MQ9BQprPC0M"
      },
      "cell_type": "markdown",
      "source": [
        "Helper function that will allows us to evaluate our model"
      ]
    },
    {
      "metadata": {
        "id": "mGQOEW4VLbuT"
      },
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataset_dict, iterations=1000, batch_size=20):\n",
        "  model.eval() # Set the model in eval mode so as ensure Dropout and BatchNorm layers operate in Evaluation mode\n",
        "\n",
        "  # Set up our dataloaders\n",
        "  dataset = OneShotDataset(dataset_dict, iterations=iterations)\n",
        "  dataset_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  num_workers=4,\n",
        "                                                  shuffle=False)\n",
        "\n",
        "  # Run in no_grad mode to speed up inference\n",
        "  with torch.no_grad():\n",
        "    # Placeholders to accumulate the metrics across batches\n",
        "    total_accuracy = 0\n",
        "    total_loss = 0\n",
        "    for iteration, (support_images, support_labels, images, labels) in enumerate(dataset_loader):\n",
        "      # Move Data to GPU\n",
        "      support_batch_torch = support_images.to(device)\n",
        "      support_labels_torch = support_labels.to(device)\n",
        "      target_images_torch = images.to(device)\n",
        "      target_labels_torch =labels.to(device)\n",
        "\n",
        "      # Perform inference via our model\n",
        "      logits, predictions = model(support_batch_torch, support_labels_torch, target_images_torch)\n",
        "      # Calculate loss and accuracy\n",
        "      loss = F.cross_entropy(logits, target_labels_torch)\n",
        "      accuracy = torch.mean((predictions == target_labels_torch).float())\n",
        "\n",
        "      # Accumulate the metrics\n",
        "      total_loss = total_loss + loss.item()\n",
        "      total_accuracy = total_accuracy + accuracy.item()\n",
        "\n",
        "    accuracy = total_accuracy/(iteration+1)\n",
        "    loss = total_loss/(iteration+1)\n",
        "    return accuracy, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o9zHIJx9KdQG"
      },
      "cell_type": "markdown",
      "source": [
        "##Baseline KNN Accuracy"
      ]
    },
    {
      "metadata": {
        "id": "yI4Qj_AqITSs"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def evaluate_one_shot_performance(dataset, nway=20, num_tests=300):\n",
        "  correct = 0\n",
        "  for i in range(num_tests):\n",
        "    support_set, support_labels, required_letter, required_label = make_oneshot_task(dataset, nway=nway)\n",
        "    images = np.insert(support_set, 0, required_letter, axis=0)\n",
        "    # Baseline model with just l2 Distance between images\n",
        "    embeddings = images.reshape(nway+1, -1)\n",
        "    neigh = KNeighborsClassifier(n_neighbors=1)\n",
        "    neigh.fit(embeddings[1:], support_labels)\n",
        "    prediction = neigh.predict(embeddings[:1])\n",
        "    if(prediction[0] == required_label):\n",
        "      correct = correct + 1\n",
        "  print(f\"Accuracy - {correct * 100 / num_tests}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vANi98TkJ1Fg"
      },
      "cell_type": "code",
      "source": [
        "evaluate_one_shot_performance(testing_dict, nway=20, num_tests=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Axa3vKahH1uW"
      },
      "cell_type": "markdown",
      "source": [
        "#Training our model"
      ]
    },
    {
      "metadata": {
        "id": "uAW_F_oQHxd1"
      },
      "cell_type": "markdown",
      "source": [
        "##Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "mNLyMZRzJ2Tk"
      },
      "cell_type": "code",
      "source": [
        "epochs = 60 #@param {type: \"slider\", min: 1, max: 100}\n",
        "batch_size = 20 #@param {type: \"slider\", min: 20, max: 200}\n",
        "iterations = 10000 #@param {type: \"slider\", min: 500, max: 10000}\n",
        "lr = 0.0001 #@param [\"0.1\", \"0.01\", \"0.001\", \"0.0001\"] {type:\"raw\", allow-input: true}\n",
        "embedding_size = 64 #@param {type: \"slider\", min: 64, max: 512}\n",
        "dropout_probality=0.1 #@param {type: \"slider\", min: 0, max: 1, step: 0.1}\n",
        "use_fce = True #@param [\"True\", \"False\"] {type:\"raw\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3uAlr5cRIP6s"
      },
      "cell_type": "markdown",
      "source": [
        "##Setting up logging for our training"
      ]
    },
    {
      "metadata": {
        "id": "3OKBnMEtQalU"
      },
      "cell_type": "markdown",
      "source": [
        "We will now setup an experiment via comet.ml library. This will allow us to keep track of multiple runs of our code and the hyperparameters used in each iteration.\n",
        "It also generates the loss and accuracy graphs over time.\n",
        "We are mostly interested in logging the\n",
        "\n",
        "1.   Train loss\n",
        "2.   Train accuracy\n",
        "3.   Dev loss\n",
        "4.   Dev Accuracy\n",
        "5.   Model Performance (Test Accuracy)\n",
        "5.   Model loss (Test loss)\n"
      ]
    },
    {
      "metadata": {
        "id": "7bKU4FwbWAau"
      },
      "cell_type": "code",
      "source": [
        "pytorch_experiment = Experiment(api_key=\"SCmpLSERj7defIsfWhQcs7D4E\",\n",
        "                        project_name=\"PyTorch Matching Net\", workspace=\"ramesharvind\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qQuOLUOgL_HJ"
      },
      "cell_type": "code",
      "source": [
        "pytorch_experiment.log_multiple_params({\"Epochs\":epochs, \"Batch Size\": batch_size, \"Iterations Per Epoch\": iterations, \"Learning Rate\": lr, \"Embedding Size\": embedding_size, \"Dropout\": dropout_probality, \"Using FCE\": use_fce})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lOEJBsjiH6ls"
      },
      "cell_type": "markdown",
      "source": [
        "##Training Loop"
      ]
    },
    {
      "metadata": {
        "id": "jvioaFnoJa-z"
      },
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "matching_net = MatchingNet(img_shape, embedding_size=embedding_size, dropout_probality=dropout_probality, use_fce=use_fce)\n",
        "print(\"Model Summary\")\n",
        "print(matching_net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pr3IHrOWbZ3Y"
      },
      "cell_type": "code",
      "source": [
        "# Move the model to GPU\n",
        "matching_net.to(device)\n",
        "# Create the optimizer\n",
        "optimizer = torch.optim.Adam(matching_net.parameters(), lr=lr)\n",
        "# placeholders to store our best performing metrics on Dev Set\n",
        "best_dev_loss = 0\n",
        "best_dev_accuracy = 0\n",
        "\n",
        "print(\"Beginning Training..\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  matching_net.train() # Set our model in train mode\n",
        "\n",
        "  # Placeholder variables to help track epoch loss and accuracy\n",
        "  total_loss = 0\n",
        "  total_accuracy = 0\n",
        "\n",
        "  # Define our dataset and dataloader\n",
        "  train_dataset = OneShotDataset(augmented_train_dict, iterations)\n",
        "  train_dataset_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  num_workers=4,\n",
        "                                                  shuffle=False)\n",
        "  # Main training loop\n",
        "  for iteration, (support_images, support_labels, images, labels) in enumerate(train_dataset_loader):\n",
        "    # Move our data to GPU\n",
        "    support_batch_torch = support_images.to(device)\n",
        "    support_labels_torch = support_labels.to(device)\n",
        "    target_images_torch = images.to(device)\n",
        "    target_labels_torch =labels.to(device)\n",
        "    # Get the predictions and logits from our model\n",
        "    logits, predictions = matching_net(support_batch_torch, support_labels_torch, target_images_torch)\n",
        "\n",
        "    # Calculate loss and accuracy in current iteration\n",
        "    loss = F.cross_entropy(logits, target_labels_torch)\n",
        "    accuracy = torch.mean((predictions == target_labels_torch).float())\n",
        "\n",
        "    # Accumulate the values\n",
        "    total_loss = total_loss + loss.item()\n",
        "    total_accuracy = total_accuracy + accuracy.item()\n",
        "\n",
        "    # Perform Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  total_accuracy = total_accuracy/(iteration+1)\n",
        "  total_loss = total_loss/(iteration+1)\n",
        "\n",
        "  # Log the training metrics to comet.ml\n",
        "  with pytorch_experiment.train():\n",
        "    pytorch_experiment.log_current_epoch(epoch)\n",
        "    pytorch_experiment.log_metric(\"loss\",total_loss)\n",
        "    pytorch_experiment.log_metric(\"accuracy\",total_accuracy)\n",
        "  print(f\"In epoch - {epoch} Train Set - Accuracy {total_accuracy} Loss - {total_loss} - for {(iteration+1)} iterations\")\n",
        "\n",
        "\n",
        "  # Run the model on Dev Set to evaluate performance on unseen data (every 3 epochs)\n",
        "  if epoch%3 == 0:\n",
        "    with pytorch_experiment.validate():\n",
        "      pytorch_experiment.log_current_epoch(epoch)\n",
        "      dev_accuracy, dev_loss = evaluate_model(matching_net, augmented_dev_dict)\n",
        "      print(f\"--In epoch - {epoch} Dev Set Accuracy - {dev_accuracy} Loss - {dev_loss}\")\n",
        "\n",
        "      # Save the best performing model across all the epochs\n",
        "      if not best_dev_loss or dev_loss < best_dev_loss:\n",
        "        print(f\"---Found Better Model to save with Accuracy - {dev_accuracy} and loss - {dev_loss}\")\n",
        "        best_dev_loss = dev_loss\n",
        "        best_dev_accuracy = dev_accuracy\n",
        "        torch.save(matching_net.state_dict(), \"matching_net-%0.2f-accuracy.pt\"%(best_dev_accuracy))\n",
        "\n",
        "      # Log the Dev metrics to comet.ml\n",
        "      pytorch_experiment.log_metric(\"accuracy\",dev_accuracy)\n",
        "      pytorch_experiment.log_metric(\"dev_loss\",dev_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GT5xu6IQH95t"
      },
      "cell_type": "markdown",
      "source": [
        "##Evaluating our model with the Test Data"
      ]
    },
    {
      "metadata": {
        "id": "EMVDyTxeOfMP"
      },
      "cell_type": "code",
      "source": [
        "with pytorch_experiment.test():\n",
        "  matching_net_test = MatchingNet(img_shape, embedding_size=embedding_size, dropout_probality=dropout_probality, use_fce=use_fce)\n",
        "  matching_net_test.load_state_dict(torch.load(\"matching_net-%0.2f-accuracy.pt\"%(best_dev_accuracy)))\n",
        "  matching_net_test.to(device)\n",
        "  test_accuracy, test_loss = evaluate_model(matching_net_test, testing_dict, iterations=5000)\n",
        "  pytorch_experiment.log_metric(\"accuracy\",test_accuracy)\n",
        "  pytorch_experiment.log_metric(\"test_loss\",test_loss)\n",
        "  print(f\"Test Set Accuracy - {test_accuracy} Loss - {test_loss}\")\n",
        "pytorch_experiment.end()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-BtPdnZF39KX"
      },
      "cell_type": "markdown",
      "source": [
        "# Pre-trained Models"
      ]
    },
    {
      "metadata": {
        "id": "eBYz9lru42BR"
      },
      "cell_type": "markdown",
      "source": [
        "## Downloading our weights"
      ]
    },
    {
      "metadata": {
        "id": "HONSuxvB2-kP"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/RameshArvind/Pytorch-Matching-Networks.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OP1hG3lg3dF7"
      },
      "cell_type": "code",
      "source": [
        "model_weights_with_fce = \"Pytorch-Matching-Networks/matching_net-FCE-0.73-accuracy.pt\"\n",
        "model_weights_without_fce = \"Pytorch-Matching-Networks/matching_net-WITHOUT-FCE-0.74-accuracy.pt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZOTufWqm4BqM"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Without FCE"
      ]
    },
    {
      "metadata": {
        "id": "1XrKQXGz3J1P"
      },
      "cell_type": "code",
      "source": [
        "embedding_size = 64\n",
        "dropout_probality=0.1\n",
        "use_fce = False\n",
        "\n",
        "matching_net_test = MatchingNet(img_shape, embedding_size=embedding_size, dropout_probality=dropout_probality, use_fce=use_fce)\n",
        "matching_net_test.load_state_dict(torch.load(model_weights_without_fce))\n",
        "matching_net_test.to(device)\n",
        "test_accuracy, test_loss = evaluate_model(matching_net_test, testing_dict, iterations=5000)\n",
        "pytorch_experiment.log_metric(\"accuracy\",test_accuracy)\n",
        "pytorch_experiment.log_metric(\"test_loss\",test_loss)\n",
        "print(f\"Without FCE - Test Set Accuracy - {test_accuracy} Loss - {test_loss}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x9Cqt13Q4JPB"
      },
      "cell_type": "markdown",
      "source": [
        "## Model With FCE"
      ]
    },
    {
      "metadata": {
        "id": "mJErbdR74Off"
      },
      "cell_type": "code",
      "source": [
        "embedding_size = 64\n",
        "dropout_probality=0.1\n",
        "use_fce = True\n",
        "\n",
        "matching_net_test = MatchingNet(img_shape, embedding_size=embedding_size, dropout_probality=dropout_probality, use_fce=use_fce)\n",
        "matching_net_test.load_state_dict(torch.load(model_weights_with_fce))\n",
        "matching_net_test.to(device)\n",
        "test_accuracy, test_loss = evaluate_model(matching_net_test, testing_dict, iterations=5000)\n",
        "pytorch_experiment.log_metric(\"accuracy\",test_accuracy)\n",
        "pytorch_experiment.log_metric(\"test_loss\",test_loss)\n",
        "print(f\"With FCE - Test Set Accuracy - {test_accuracy} Loss - {test_loss}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E8o_Xokt3_zn"
      },
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "We are able to train a model that can solve the one shot 20-way classification problem to a reasonable degree with about **~73% with Fully Conditional Embedding** (and **~77% accuracy without it**) on the test set.\n",
        "\n",
        "Some of the approaches we can try to reach the reported 93.8% results are -\n",
        "\n",
        "\n",
        "1.   Use image augmentation\n",
        "2.   Automated Hyperparameter optimization\n",
        "3.   Utilize correct implmentation of the Fully conditional embedding for the target image.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "CZ-O4NLzfxY1"
      },
      "cell_type": "markdown",
      "source": [
        "#TODO Items\n",
        "###<s>Make dataloader - To speed up training</s> - DONE - Loading all the numpy images at start speeds up things by a TON\n",
        "###<s>Figure out what optimizer was used - SGD/ADAM? </s>- ADAM WORKS\n",
        "### <s>Use image augmentation </s> - Done\n",
        "###<s>There is still something wrong with the network - Probably something with the way augmentation is done?</s> There isn't a problem with augmentation, rather the training and testing set provided by default have different \"distributions\". Need to investigate this further. Fixed by mixing the provided test and train set and creating new Train/Dev/Test sets.\n",
        "###<s>Setup a way to log training of models</s> - Done via integraion with comet.ml\n",
        "###Setup Hyperparameter Optimization\n",
        "###Use a high level pytorch API to setup training loops and Checkpointing - PyTorch Ignite\n",
        "###Using a dense layer at the end during embedding was causing the network to converge faser and better - Figure out why\n",
        "###<s>Work on the LSTM based embedding</s> - Done Albeit incorrectly\n",
        "### Implement Custom LSTM Cell which has hidden weights which can accept concatenated input\n"
      ]
    }
  ]
}